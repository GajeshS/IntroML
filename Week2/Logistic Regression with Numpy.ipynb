{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Numpy\n",
    "\n",
    "Here we'll attempt to classify the Iris Flower Dataset with a model that we'll build ourselves. \n",
    "\n",
    "**Note:** We won't be using any frameworks, we'll just use numpy and plain python.\n",
    "\n",
    "\n",
    "## The Iris flower data set / Fisher's Iris data set\n",
    " - https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    " - Multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper *The use of multiple measurements in taxonomic problems* as an example of linear discriminant analysis.\n",
    " - Input / Feature Names (150 x 4)\n",
    "     - sepal length (cm)\n",
    "     - sepal width (cm)\n",
    "     - petal length (cm)\n",
    "     - petal width (cm)\n",
    " - Output (150 x 1)\n",
    "    - Iris species label\n",
    "        0. setosa\n",
    "        1. versicolor\n",
    "        2. virginica\n",
    "\n",
    "- Aim : \n",
    "    - Classifying an Iris flower into one of these species based on features\n",
    "    - ie. Assign a class to given input sample\n",
    "\n",
    "So as we can see, the dataset consists of **4 input features** and the flowers can be classified as **1 of 3 categorires** as specified above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependancies\n",
    "\n",
    "So here are the essentials we'll need to perform the above classification.\n",
    "\n",
    "*Additionally we'll use matplotlib to visualize the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "The dataset comes with sklearn as a standard dataset. So we'll use the datasets module within sklearn to load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 150)\n",
      "(1, 150)\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data.T # Here we take the transposition of the data matrix. \n",
    "y = iris.target.reshape((1,150))\n",
    "\n",
    "print X.shape\n",
    "print y.shape\n",
    "print type(y)\n",
    "order = np.random.permutation(len(X[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "One-hot or One-of-k is a method we'll use to encode our classes such that **no class has any features that will distinctly differentiate them** and **to ensure all the y-values lie between 0 and 1**.\n",
    "\n",
    "Hence this is how our data will be modified.\n",
    "\n",
    "|Original Value| One-Hot Encoded|\n",
    "|--------------|----------------|\n",
    "|0| [1, 0, 0]|\n",
    "|1| [0, 1, 0]|\n",
    "|2| [0, 0, 1]|\n",
    "\n",
    "Refer [sklearn OneHotEncoder docs](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) for more information. Here since our dataset is small we'll just one-hot encode with a small Python Script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "ohe_y = np.array([[0,0,0]])\n",
    "\n",
    "for datapoint in y[0]:\n",
    "    \n",
    "    if datapoint == 0:\n",
    "        ohe_y = np.vstack([ohe_y, np.array([1,0,0])])\n",
    "    elif datapoint == 1:\n",
    "        ohe_y = np.vstack([ohe_y, np.array([0,1,0])])\n",
    "    else:\n",
    "        ohe_y = np.vstack([ohe_y, np.array([0,0,1])])\n",
    "        \n",
    "ohe_y = ohe_y[1:,:]\n",
    "print np.shape(ohe_y)\n",
    "assert(np.sum(ohe_y, axis=1).all() == 1) # Sanity Check: We're checking that each tuple adds to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Training and Testing Data\n",
    "\n",
    "Now that we've set up our data let's split it up into **Testing** and **Training** sets.\n",
    "\n",
    "As we've explained before to proplerly validate our model, we need to ensure that it's trained and tested on mutually exclusive sets of data. \n",
    "\n",
    "**Why?**\n",
    "\n",
    "If we train a model on a set of data, the model will understand the patterns within that dataset and may even perform well during training. But since it's already fit to that data the testing accuracy will be high. Hence we need to ensure that the model is tested on unseen data so as to properly understand it's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape\n",
      "(4, 120)\n",
      "(120, 3)\n",
      "Test dataset shape\n",
      "(4, 30)\n",
      "(30, 3)\n"
     ]
    }
   ],
   "source": [
    "# Shuffling data\n",
    "order = np.random.permutation(len(X[1]))\n",
    "portion = 30\n",
    "\n",
    "# Splitting data into train and test \n",
    "# samples 0-19  : Test set\n",
    "# samples 20-99 : Train set\n",
    "test_x = X[:,order[:portion]]\n",
    "test_y = ohe_y[order[:portion],:]\n",
    "train_x = X[:,order[portion:]]\n",
    "train_y = ohe_y[order[portion:],:]\n",
    "\n",
    "print (\"Training dataset shape\")\n",
    "print (train_x.shape)\n",
    "print (train_y.shape)\n",
    "\n",
    "print (\"Test dataset shape\")\n",
    "print (test_x.shape)\n",
    "print (test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "Now that we've split our training data into training and testing sets let's move on to the actual model. \n",
    "An activation function, as we've explained in class the activation function will define the output of our logit using our weighted features as input.\n",
    "\n",
    "For a list of various activation functions and their properties [click here](https://en.wikipedia.org/wiki/Activation_function#Comparison%20of%20activation%20functions).\n",
    "\n",
    "We need an activation function that:\n",
    "\n",
    "* Flattens the output between the range 0 and 1\n",
    "* Has very small range within which the output is neither close to 0 or 1.\n",
    "\n",
    "Hence we'll use the Sigmoid activation function.\n",
    "It looks like this:\n",
    "\n",
    "![image](https://cdn-images-1.medium.com/max/1200/1*Vo7UFksa_8Ne5HcfEzHNWQ.png)\n",
    "\n",
    "In the next cell we'll demonstrate how this function maps an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_x = np.linspace(-20,20,100)\n",
    "plt.plot(sample_x,sigmoid(sample_x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence to implement in python we just need to define the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "The output for a given sample is derived using the following formula\n",
    "![Output](https://cdn-images-1.medium.com/max/800/1*p4hYc2VwJqoLWwl_mV0Vjw.png)\n",
    "\n",
    "Here, \n",
    "- `g` - Sigmoid Function\n",
    "- `theta` - Weight Matrix\n",
    "- `z` - Output\n",
    "- `h` - Activated Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,theta):\n",
    "    z = (theta.T).dot(x)\n",
    "    hx = sigmoid(z)\n",
    "    return hx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "**REFER : https://www.youtube.com/watch?v=K0YBDyxEXKE**\n",
    "![Loss](https://cdn-images-1.medium.com/max/800/1*FdxEs8Iv_43Q8calTCjnow.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(h, y):\n",
    "    return np.sum(np.negative(y) * np.log(h) + (1 - y) * (np.log(1 - h))) / y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "![Grad formula](https://cdn-images-1.medium.com/max/800/1*gobKgGbRWDAoVFAan_HjxQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,h,y):\n",
    "#     print (X.T.shape)\n",
    "#     print ((h-y).shape)\n",
    "    return (1.0 / y.shape[1]) * np.dot(X, (h - y).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(theta, b, x, y):\n",
    "    y_estimate = predict(x,theta)\n",
    "    error = loss(y_estimate,y)\n",
    "    grad = gradient(x,y_estimate,y)\n",
    "    error = np.squeeze(error)\n",
    "    \n",
    "    return grad,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "(4, 1)\n",
      "Iteration: 50 - Error: 0.7239424394876124\n",
      "Iteration: 100 - Error: 0.7200409318270575\n",
      "Iteration: 150 - Error: 0.716178945511507\n",
      "Iteration: 200 - Error: 0.7123561912361244\n",
      "Iteration: 250 - Error: 0.7085723799281088\n",
      "Iteration: 300 - Error: 0.7048272227987373\n",
      "Iteration: 350 - Error: 0.7011204313942895\n",
      "Iteration: 400 - Error: 0.697451717645851\n",
      "Iteration: 450 - Error: 0.6938207939179997\n",
      "Iteration: 500 - Error: 0.6902273730563733\n",
      "Iteration: 550 - Error: 0.6866711684341226\n",
      "Iteration: 600 - Error: 0.6831518939972497\n",
      "Iteration: 650 - Error: 0.6796692643088391\n",
      "Iteration: 700 - Error: 0.6762229945921826\n",
      "Iteration: 750 - Error: 0.6728128007728041\n",
      "Iteration: 800 - Error: 0.6694383995193894\n",
      "Iteration: 850 - Error: 0.6660995082836287\n",
      "Iteration: 900 - Error: 0.6627958453389767\n",
      "Iteration: 950 - Error: 0.6595271298183383\n",
      "Iteration: 1000 - Error: 0.6562930817506886\n",
      "Iteration: 1050 - Error: 0.6530934220966351\n",
      "Iteration: 1100 - Error: 0.6499278727829306\n",
      "Iteration: 1150 - Error: 0.6467961567359474\n",
      "Iteration: 1200 - Error: 0.6436979979141212\n",
      "Iteration: 1250 - Error: 0.6406331213393774\n",
      "Iteration: 1300 - Error: 0.6376012531275475\n",
      "Iteration: 1350 - Error: 0.6346021205177885\n",
      "Iteration: 1400 - Error: 0.631635451901018\n",
      "Iteration: 1450 - Error: 0.6287009768473731\n",
      "Iteration: 1500 - Error: 0.6257984261327096\n",
      "Iteration: 1550 - Error: 0.6229275317641499\n",
      "Iteration: 1600 - Error: 0.6200880270046959\n",
      "Iteration: 1650 - Error: 0.617279646396916\n",
      "Iteration: 1700 - Error: 0.6145021257857237\n",
      "Iteration: 1750 - Error: 0.6117552023402573\n",
      "Iteration: 1800 - Error: 0.609038614574876\n",
      "Iteration: 1850 - Error: 0.606352102369286\n",
      "Iteration: 1900 - Error: 0.6036954069878117\n",
      "Iteration: 1950 - Error: 0.6010682710978216\n",
      "Iteration: 2000 - Error: 0.5984704387873294\n",
      "Iteration: 2050 - Error: 0.5959016555817792\n",
      "Iteration: 2100 - Error: 0.5933616684600317\n",
      "Iteration: 2150 - Error: 0.5908502258695644\n",
      "Iteration: 2200 - Error: 0.588367077740902\n",
      "Iteration: 2250 - Error: 0.5859119755012908\n",
      "Iteration: 2300 - Error: 0.5834846720876286\n",
      "Iteration: 2350 - Error: 0.5810849219586698\n",
      "Iteration: 2400 - Error: 0.578712481106517\n",
      "Iteration: 2450 - Error: 0.5763671070674127\n",
      "Iteration: 2500 - Error: 0.5740485589318488\n",
      "Iteration: 2550 - Error: 0.5717565973540052\n",
      "Iteration: 2600 - Error: 0.5694909845605337\n",
      "Iteration: 2650 - Error: 0.5672514843587009\n",
      "Iteration: 2700 - Error: 0.565037862143904\n",
      "Iteration: 2750 - Error: 0.5628498849065747\n",
      "Iteration: 2800 - Error: 0.5606873212384842\n",
      "Iteration: 2850 - Error: 0.5585499413384631\n",
      "Iteration: 2900 - Error: 0.5564375170175528\n",
      "Iteration: 2950 - Error: 0.5543498217035968\n",
      "Iteration: 3000 - Error: 0.5522866304452921\n",
      "Iteration: 3050 - Error: 0.5502477199157082\n",
      "Iteration: 3100 - Error: 0.5482328684152925\n",
      "Iteration: 3150 - Error: 0.54624185587437\n",
      "Iteration: 3200 - Error: 0.5442744638551557\n",
      "Iteration: 3250 - Error: 0.5423304755532874\n",
      "Iteration: 3300 - Error: 0.5404096757988982\n",
      "Iteration: 3350 - Error: 0.5385118510572331\n",
      "Iteration: 3400 - Error: 0.5366367894288309\n",
      "Iteration: 3450 - Error: 0.5347842806492771\n",
      "Iteration: 3500 - Error: 0.5329541160885432\n",
      "Iteration: 3550 - Error: 0.5311460887499245\n",
      "Iteration: 3600 - Error: 0.5293599932685864\n",
      "Iteration: 3650 - Error: 0.5275956259097324\n",
      "Iteration: 3700 - Error: 0.5258527845664056\n",
      "Iteration: 3750 - Error: 0.5241312687569329\n",
      "Iteration: 3800 - Error: 0.5224308796220257\n",
      "Iteration: 3850 - Error: 0.5207514199215468\n",
      "Iteration: 3900 - Error: 0.519092694030953\n",
      "Iteration: 3950 - Error: 0.5174545079374273\n",
      "Iteration: 4000 - Error: 0.5158366692357068\n",
      "Iteration: 4050 - Error: 0.5142389871236215\n",
      "Iteration: 4100 - Error: 0.5126612723973492\n",
      "Iteration: 4150 - Error: 0.5111033374463995\n",
      "Iteration: 4200 - Error: 0.509564996248336\n",
      "Iteration: 4250 - Error: 0.5080460643632451\n",
      "Iteration: 4300 - Error: 0.5065463589279613\n",
      "Iteration: 4350 - Error: 0.5050656986500596\n",
      "Iteration: 4400 - Error: 0.5036039038016208\n",
      "Iteration: 4450 - Error: 0.5021607962127816\n",
      "Iteration: 4500 - Error: 0.5007361992650764\n",
      "Iteration: 4550 - Error: 0.49932993788457836\n",
      "Iteration: 4600 - Error: 0.4979418385348513\n",
      "Iteration: 4650 - Error: 0.49657172920971615\n",
      "Iteration: 4700 - Error: 0.49521943942584307\n",
      "Iteration: 4750 - Error: 0.49388480021517506\n",
      "Iteration: 4800 - Error: 0.49256764411719234\n",
      "Iteration: 4850 - Error: 0.49126780517102187\n",
      "Iteration: 4900 - Error: 0.4899851189074036\n",
      "Iteration: 4950 - Error: 0.4887194223405151\n",
      "Iteration: 5000 - Error: 0.4874705539596664\n",
      "Iteration: 5050 - Error: 0.48623835372086843\n",
      "Iteration: 5100 - Error: 0.4850226630382824\n",
      "Iteration: 5150 - Error: 0.4838233247755577\n",
      "Iteration: 5200 - Error: 0.4826401832370623\n",
      "Iteration: 5250 - Error: 0.48147308415901346\n",
      "Iteration: 5300 - Error: 0.48032187470051396\n",
      "Iteration: 5350 - Error: 0.4791864034344992\n",
      "Iteration: 5400 - Error: 0.47806652033860136\n",
      "Iteration: 5450 - Error: 0.47696207678593605\n",
      "Iteration: 5500 - Error: 0.4758729255358159\n",
      "Iteration: 5550 - Error: 0.47479892072439683\n",
      "Iteration: 5600 - Error: 0.47373991785526254\n",
      "Iteration: 5650 - Error: 0.4726957737899505\n",
      "Iteration: 5700 - Error: 0.47166634673842556\n",
      "Iteration: 5750 - Error: 0.47065149624950564\n",
      "Iteration: 5800 - Error: 0.4696510832012424\n",
      "Iteration: 5850 - Error: 0.46866496979126365\n",
      "Iteration: 5900 - Error: 0.46769301952707953\n",
      "Iteration: 5950 - Error: 0.46673509721635764\n",
      "Iteration: 6000 - Error: 0.4657910689571712\n",
      "Iteration: 6050 - Error: 0.4648608021282226\n",
      "Iteration: 6100 - Error: 0.46394416537904815\n",
      "Iteration: 6150 - Error: 0.46304102862020585\n",
      "Iteration: 6200 - Error: 0.46215126301345105\n",
      "Iteration: 6250 - Error: 0.46127474096190113\n",
      "Iteration: 6300 - Error: 0.4604113361001964\n",
      "Iteration: 6350 - Error: 0.45956092328465536\n",
      "Iteration: 6400 - Error: 0.45872337858343193\n",
      "Iteration: 6450 - Error: 0.4578985792666738\n",
      "Iteration: 6500 - Error: 0.4570864037966887\n",
      "Iteration: 6550 - Error: 0.45628673181811635\n",
      "Iteration: 6600 - Error: 0.45549944414811455\n",
      "Iteration: 6650 - Error: 0.45472442276655667\n",
      "Iteration: 6700 - Error: 0.4539615508062457\n",
      "Iteration: 6750 - Error: 0.45321071254314815\n",
      "Iteration: 6800 - Error: 0.4524717933866464\n",
      "Iteration: 6850 - Error: 0.45174467986981676\n",
      "Iteration: 6900 - Error: 0.4510292596397302\n",
      "Iteration: 6950 - Error: 0.4503254214477816\n",
      "Iteration: 7000 - Error: 0.4496330551400476\n",
      "Iteration: 7050 - Error: 0.44895205164767577\n",
      "Iteration: 7100 - Error: 0.4482823029773056\n",
      "Iteration: 7150 - Error: 0.447623702201526\n",
      "Iteration: 7200 - Error: 0.44697614344936604\n",
      "Iteration: 7250 - Error: 0.4463395218968266\n",
      "Iteration: 7300 - Error: 0.4457137337574487\n",
      "Iteration: 7350 - Error: 0.4450986762729243\n",
      "Iteration: 7400 - Error: 0.4444942477037482\n",
      "Iteration: 7450 - Error: 0.4439003473199141\n",
      "Iteration: 7500 - Error: 0.4433168753916547\n",
      "Iteration: 7550 - Error: 0.4427437331802287\n",
      "Iteration: 7600 - Error: 0.4421808229287539\n",
      "Iteration: 7650 - Error: 0.4416280478530886\n",
      "Iteration: 7700 - Error: 0.44108531213276414\n",
      "Iteration: 7750 - Error: 0.44055252090196373\n",
      "Iteration: 7800 - Error: 0.44002958024055705\n",
      "Iteration: 7850 - Error: 0.4395163971651837\n",
      "Iteration: 7900 - Error: 0.43901287962039043\n",
      "Iteration: 7950 - Error: 0.43851893646982243\n",
      "Iteration: 8000 - Error: 0.4380344774874692\n",
      "Iteration: 8050 - Error: 0.4375594133489647\n",
      "Iteration: 8100 - Error: 0.43709365562294406\n",
      "Iteration: 8150 - Error: 0.4366371167624564\n",
      "Iteration: 8200 - Error: 0.43618971009643487\n",
      "Iteration: 8250 - Error: 0.4357513498212241\n",
      "Iteration: 8300 - Error: 0.43532195099216575\n",
      "Iteration: 8350 - Error: 0.4349014295152426\n",
      "Iteration: 8400 - Error: 0.43448970213878096\n",
      "Iteration: 8450 - Error: 0.4340866864452141\n",
      "Iteration: 8500 - Error: 0.4336923008429032\n",
      "Iteration: 8550 - Error: 0.43330646455801997\n",
      "Iteration: 8600 - Error: 0.4329290976264892\n",
      "Iteration: 8650 - Error: 0.432560120885991\n",
      "Iteration: 8700 - Error: 0.4321994559680254\n",
      "Iteration: 8750 - Error: 0.4318470252900362\n",
      "Iteration: 8800 - Error: 0.4315027520475984\n",
      "Iteration: 8850 - Error: 0.4311665602066644\n",
      "Iteration: 8900 - Error: 0.43083837449587326\n",
      "Iteration: 8950 - Error: 0.43051812039892146\n",
      "Iteration: 9000 - Error: 0.43020572414699493\n",
      "Iteration: 9050 - Error: 0.42990111271126275\n",
      "Iteration: 9100 - Error: 0.42960421379543373\n",
      "Iteration: 9150 - Error: 0.42931495582837254\n",
      "Iteration: 9200 - Error: 0.4290332679567801\n",
      "Iteration: 9250 - Error: 0.42875908003793434\n",
      "Iteration: 9300 - Error: 0.4284923226324931\n",
      "Iteration: 9350 - Error: 0.4282329269973577\n",
      "Iteration: 9400 - Error: 0.4279808250786004\n",
      "Iteration: 9450 - Error: 0.42773594950445026\n",
      "Iteration: 9500 - Error: 0.42749823357834277\n",
      "Iteration: 9550 - Error: 0.4272676112720302\n",
      "Iteration: 9600 - Error: 0.427044017218752\n",
      "Iteration: 9650 - Error: 0.42682738670646664\n",
      "Iteration: 9700 - Error: 0.426617655671145\n",
      "Iteration: 9750 - Error: 0.42641476069012335\n",
      "Iteration: 9800 - Error: 0.42621863897551665\n",
      "Iteration: 9850 - Error: 0.42602922836769314\n",
      "Iteration: 9900 - Error: 0.42584646732880693\n",
      "Iteration: 9950 - Error: 0.42567029493639197\n",
      "Iteration: 10000 - Error: 0.425500650877015\n"
     ]
    }
   ],
   "source": [
    "def learn(alpha = 0.000004):\n",
    "    # Initialising a random vector of weights\n",
    "    theta = np.zeros((4,1))\n",
    "    b = 0\n",
    "\n",
    "    # Learning rate alpha passed as a parameter.\n",
    "    \n",
    "    # Threshold to terminate learning\n",
    "    tolerance = 1e-5\n",
    "    errorold = 0\n",
    "\n",
    "    # Perform Gradient Descent\n",
    "    iterations = 1\n",
    "    print theta\n",
    "    print theta.shape\n",
    "    while True:\n",
    "        theta_gradient, error = get_gradient(theta, b, train_x, train_y)\n",
    "        theta_new = theta - (alpha * theta_gradient)\n",
    "\n",
    "        # Stopping Condition\n",
    "        if np.sum(abs(theta_new - theta)) < tolerance:\n",
    "            print (\"Converged.\")\n",
    "            break\n",
    "\n",
    "        # Print error every 50 iterations\n",
    "        if iterations % 50 == 0:\n",
    "            print (\"Iteration: \"+str(iterations)+\" - Error: \"+ str(np.sum(error)))\n",
    "    #         break\n",
    "\n",
    "        if iterations == 10000:\n",
    "            break\n",
    "        iterations += 1\n",
    "\n",
    "    #     if (error - errorold < 0):\n",
    "    #         print \"Overshot: Iteration - \" + str(iterations)  \n",
    "    #         break\n",
    "\n",
    "        theta = theta_new\n",
    "        errorold = error\n",
    "\n",
    "    return theta, b, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's train the model:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
