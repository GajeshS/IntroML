{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've implemented Logistic Regression, let's go ahead and move on to the next logical concept.<br>\n",
    "As we'd mentioned before Neural Networks can basically be considered as **multiple logical units** (logits) stacked as multiple layers.\n",
    "\n",
    "Let's understand the **basic structure** of a Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](http://neuralnetworksanddeeplearning.com/images/tikz1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remembered our model from last week you'll actually see that it looks very much like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](static/logit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are a set of m inputs and a set of n outputs where we try to find the value of the weights give us the best possible prediction for a set of inputs. <br> And this is in fact **exactly how the first section of the Neural Network** from the first cell looks.\n",
    "\n",
    "Hence the intuition is that Logistic Regression can be **naturally extended** to build a Neural Network. In essense a Neural Network is in fact sets of *Logits* (Logical Units) that are stacked to form layers and multiple layers of the same to form a network of logits such that each logit in a layer has a set of weighted of inputs that are passed through a function to generate the output which will in turn be fed to a logit in the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers\n",
    "\n",
    "You'll notice that in the neural network one of the main differences is that the network has multiple layers of these inputs and not just the one for input and output.\n",
    "\n",
    "These layers between the input and the output layers are called **hidden layers**. These hidden layers are the magical reason neural networks work as well as they do in real-world applications.\n",
    "\n",
    "Essentially the hidden layers allow to complicate or add a degree of non-linearity to the function that the network is trying to replicate. The basic point of any model is to attempt to find a function that generates the distribution of data provided to the model as training data in the hope that any other data provided will also be correlated correctly with the same function. Hence adding hidden layers tends to increase the flexibility of the function or increases the degree to which the function we're trying to predict strays from forming lines and making simpler binary decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
