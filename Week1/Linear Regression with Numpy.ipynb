{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with `numpy`\n",
    "- [Why numpy over python](https://stackoverflow.com/questions/993984/why-numpy-instead-of-python-lists) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "Before we can start working on our actual algorithms and models, we have to import our data in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "import numpy as np\n",
    "\n",
    "data_x = np.linspace(1.0, 10.0, 100)[:, np.newaxis]\n",
    "data_y = np.sin(data_x) + 0.1*np.power(data_x,2) + 0.5*np.random.randn(100,1)\n",
    "data_x /= np.max(data_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks\n",
    "\n",
    "So now that we've imported our data we can quickly check if the data has in fact been **loadd correctly.** \n",
    "A *simple* way to do that would be to print the shape of the data we've imported. \n",
    "\n",
    "So these simple checks to ensure that bigger errors don't pop up later are called *sanity checks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 8)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "print (data_x.shape)\n",
    "print (data_y.shape)\n",
    "\n",
    "# Adding bias to x\n",
    "data_x = np.hstack((np.ones_like(data_x), data_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Testing Data\n",
    "\n",
    "When training an ML Model a best-practice is to split your available data into two sets. One for training and one for testing and evaluation. \n",
    "\n",
    "**Why?**\n",
    "\n",
    "If a model trains on a set of data and learns it's patterns well enough then it will obviously perform well on the same set of data. \n",
    "\n",
    "We should keep the sets mutually exclusive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling data\n",
    "order = np.random.permutation(len(data_x))\n",
    "portion = 20\n",
    "\n",
    "# Splitting data into train and test \n",
    "test_x = data_x[order[:portion]]\n",
    "test_y = data_y[order[:portion]]\n",
    "train_x = data_x[order[portion:]]\n",
    "train_y = data_y[order[portion:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(w, x, y):\n",
    "    y_estimate = x.dot(w).flatten()\n",
    "    # Error = expected_value - predicted_value\n",
    "    error = (y.flatten() - y_estimate)\n",
    "    gradient = -(1.0/len(x)) * error.dot(x)\n",
    "    return gradient, error**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Convergence\n",
    "\n",
    "All Machine Learning or Deep Learning models require training. Training is the phase in which your model/algorithm attempts to *learn* and *understand* the data that you've provided.\n",
    "\n",
    "The **learning rate** is the rate at which your algorithm learns the data. We'll get into detail about this later but for now we can just say that the learning rate should neither be too high, or too small for good training.\n",
    "\n",
    "**Convergence** occurs when you've *finished* training your data. Technically there's no way to define when training is done but in essence we define convergence to have occured when your weights don't change by some threshold value between two successive training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100 - Error: 113.35754457226828\n",
      "Iteration: 200 - Error: 111.52822837248205\n",
      "Iteration: 300 - Error: 111.51988941729705\n",
      "Converged.\n"
     ]
    }
   ],
   "source": [
    "# Initialising a random vector of weights\n",
    "w = np.random.randn(2)\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.5\n",
    "\n",
    "# Threshold to terminate learning\n",
    "tolerance = 1e-5\n",
    "\n",
    "# Perform Gradient Descent\n",
    "iterations = 1\n",
    "while True:\n",
    "    gradient, error = get_gradient(w, train_x, train_y)\n",
    "    new_w = w - alpha * gradient\n",
    "    \n",
    "    # Stopping Condition\n",
    "    if np.sum(abs(new_w - w)) < tolerance:\n",
    "        print (\"Converged.\")\n",
    "        break\n",
    "    \n",
    "    # Print error every 50 iterations\n",
    "    if iterations % 100 == 0:\n",
    "        print (\"Iteration: \"+str(iterations)+\" - Error: \"+ str(np.sum(error)))\n",
    "    \n",
    "    iterations += 1\n",
    "    w = new_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
